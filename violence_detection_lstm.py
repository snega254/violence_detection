# -*- coding: utf-8 -*-
"""violence_detection_lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IfRIZS2LXag5kxt4EuLHfRIQbKFu69sc
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q tensorflow opencv-python scikit-learn tqdm

import os, glob, random, cv2, numpy as np, matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras import layers, models

SEED = 42
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# =============================
# 1) Config
# =============================
DATA_DIR = "/content/drive/MyDrive/Real Life Violence Dataset"   # <--- change
CLASSES  = ["NonViolence", "Violence"]     # <--- change to your folder names

TEST_SIZE = 0.20
MAX_VIDS_PER_CLASS = None   # use all (set to e.g., 300 to speed up)
N_FRAMES = 32
IMG_SIZE = 160
BATCH_SIZE = 8
EPOCHS = 12

FEATURES_DIR = "/content/drive/MyDrive/Real Life Violence Dataset"
os.makedirs(FEATURES_DIR, exist_ok=True)
X_TRAIN_FILE = os.path.join(FEATURES_DIR, "X_train.npy")
Y_TRAIN_FILE = os.path.join(FEATURES_DIR, "y_train.npy")
X_TEST_FILE  = os.path.join(FEATURES_DIR, "X_test.npy")
Y_TEST_FILE  = os.path.join(FEATURES_DIR, "y_test.npy")

# =============================
# 2) Gather files & 80/20 split
# =============================
all_paths, all_labels = [], []
for li, cls in enumerate(CLASSES):
    paths = sorted(glob.glob(os.path.join(DATA_DIR, cls, "*")))
    if MAX_VIDS_PER_CLASS: paths = paths[:MAX_VIDS_PER_CLASS]
    all_paths.extend(paths)
    all_labels.extend([li]*len(paths))

print("Total videos:", len(all_paths))

train_paths, test_paths, train_labels, test_labels = train_test_split(
    all_paths, all_labels, test_size=TEST_SIZE, stratify=all_labels, random_state=SEED
)

print(f"Train: {len(train_paths)} | Test: {len(test_paths)}")

# =============================
# Save split info to Drive
# =============================
SPLIT_SAVE_DIR = "/content/drive/MyDrive/Violence-Split"
os.makedirs(SPLIT_SAVE_DIR, exist_ok=True)

np.save(os.path.join(SPLIT_SAVE_DIR, "train_paths.npy"), np.array(train_paths))
np.save(os.path.join(SPLIT_SAVE_DIR, "train_labels.npy"), np.array(train_labels))
np.save(os.path.join(SPLIT_SAVE_DIR, "test_paths.npy"), np.array(test_paths))
np.save(os.path.join(SPLIT_SAVE_DIR, "test_labels.npy"), np.array(test_labels))

print("Saved train/test split info to:", SPLIT_SAVE_DIR)

# =============================
# 3) Frame sampler & visualize
# =============================
def sample_frames_from_video(path, n_frames=N_FRAMES, img_size=IMG_SIZE):
    cap = cv2.VideoCapture(path)
    if not cap.isOpened():
        print(f"Error: Could not open video file: {path}")
        return None
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total <= 0:
        print(f"Error: Video file has no frames: {path}")
        cap.release(); return None
    idxs = np.linspace(0, max(total-1, 0), n_frames, dtype=int)
    frames, i = [], 0
    while True:
        ret, frame = cap.read()
        if not ret: break
        if i in idxs:
            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (img_size, img_size))
            frames.append(img)
        i += 1
    cap.release()
    if len(frames) != n_frames:
        return None
    return np.array(frames, dtype=np.uint8)

# visualize 1 random video per class
for cls in CLASSES:
    cands = [p for p in all_paths if os.path.basename(os.path.dirname(p)) == cls]
    if not cands: continue
    vp = random.choice(cands)
    frs = sample_frames_from_video(vp)
    if frs is None:
        print("Skip visualize:", vp)
        continue
    cols = 4
    rows = int(np.ceil(N_FRAMES/cols))
    plt.figure(figsize=(12,6)); plt.suptitle(f"{cls}: {os.path.basename(vp)}")
    for i in range(N_FRAMES):
        plt.subplot(rows, cols, i+1)
        plt.imshow(frs[i]); plt.axis("off")
    plt.tight_layout(); plt.show()

# =============================
# 4) CNN (frozen) + feature extractor
# =============================
base_cnn = tf.keras.applications.MobileNetV2(
    include_top=False, weights='imagenet', pooling='avg', input_shape=(IMG_SIZE, IMG_SIZE, 3)
)
base_cnn.trainable = False
preprocess_fn = tf.keras.applications.mobilenet_v2.preprocess_input

@tf.function(reduce_retracing=True)
def cnn_forward(frames):
    frames = tf.cast(frames, tf.float32)
    frames = preprocess_fn(frames)
    feats = base_cnn(frames, training=False)
    return feats

def extract_features_for_path(path):
    frames = sample_frames_from_video(path)
    if frames is None: return None
    feats = cnn_forward(frames).numpy()   # (N_FRAMES, feat_dim)
    return feats

def build_or_load_features(paths, labels, x_file, y_file):
    if os.path.exists(x_file) and os.path.exists(y_file):
        print("Loading cached:", x_file)
        X = np.load(x_file, mmap_mode='r')
        y = np.load(y_file, mmap_mode='r')
        return X, y

    X_list, y_list = [], []
    for p, lab in tqdm(list(zip(paths, labels)), total=len(paths), desc="Extracting features"):
        feats = extract_features_for_path(p)
        if feats is None: continue
        X_list.append(feats)
        y_list.append(lab)
    X = np.array(X_list)
    y = np.array(y_list)
    np.save(x_file, X); np.save(y_file, y)
    return X, y

# =============================
# 5) Feature caching (train & test)
# =============================
X_train, y_train = build_or_load_features(train_paths, train_labels, X_TRAIN_FILE, Y_TRAIN_FILE)
X_test,  y_test  = build_or_load_features(test_paths,  test_labels,  X_TEST_FILE,  Y_TEST_FILE)

print("Train feats:", X_train.shape, y_train.shape)
print("Test  feats:", X_test.shape,  y_test.shape)

# =============================
# 6) CNN+LSTM model (train only LSTM head)
# =============================
timesteps  = X_train.shape[1]
feat_dim   = X_train.shape[2]
n_classes  = len(CLASSES)

inputs = layers.Input(shape=(timesteps, feat_dim))
x = layers.Masking()(inputs)
x = layers.LSTM(128)(x)
x = layers.Dropout(0.4)(x)
x = layers.Dense(64, activation='relu')(x)
outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)

model = models.Model(inputs, outputs)
model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

# =============================
# 7) Train
# =============================
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor='val_accuracy'),
    tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.2, min_lr=1e-6)
]

history = model.fit(
    X_train, y_train,
    validation_split=0.15,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
)

# =============================
# 8) Evaluate (normal)
# =============================
test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)
print(f"Test accuracy: {test_acc:.4f}")

probs = model.predict(X_test, batch_size=BATCH_SIZE)
preds = np.argmax(probs, axis=1)
print(classification_report(y_test, preds, target_names=CLASSES))
print("Confusion Matrix:\n", confusion_matrix(y_test, preds))

# =============================
# 9) Save
# =============================
SAVE_DIR = "/content/drive/MyDrive/violence_cnn_lstm_cached"
os.makedirs(SAVE_DIR, exist_ok=True)
model.save(os.path.join(SAVE_DIR, "model.keras"))
print("Saved at:", SAVE_DIR)

def predict_and_show_video(input_path, threshold=0.75):
    # Step 1: Convert for Colab playback
    converted_path = convert_video_for_colab(input_path)

    # Step 2: Predict
    pred = predict_video(converted_path)
    label = max(pred, key=pred.get)
    score = pred[label]

    # Step 3: Enforce new threshold logic
    if score >= threshold:
        final_label = label
        print(f"âœ… Prediction: {final_label} ({score*100:.2f}%) ")
    else:
        # Return the opposite class if confidence is low
        other_label = [cls for cls in CLASSES if cls != label][0]
        final_label = other_label
        print(f" Prediction: {final_label} ({score*100:.2f}%)")

    # Step 4: Show video
    return IPVideo(converted_path, embed=True, width=500)

custom_path = "/content/drive/MyDrive/Real Life Violence Dataset/NonViolence/NV_9.mp4"
predict_and_show_video(custom_path)

custom_path = "/content/drive/MyDrive/Real Life Violence Dataset/Violence/V_11.mp4"
predict_and_show_video(custom_path)

custom_path = "/content/drive/MyDrive/Real Life Violence Dataset/Violence/V_2.mp4"
predict_and_show_video(custom_path)

custom_path = "/content/drive/MyDrive/test/test1.mp4"
predict_and_show_video(custom_path)

custom_path = "/content/drive/MyDrive/test/test2.mp4"
predict_and_show_video(custom_path)

# =============================
# 11) Live camera (run locally, not Colab hosted)
# =============================
def predict_live(cam_index=0):
    cap = cv2.VideoCapture(cam_index)
    buf = []
    while True:
        ret, frame = cap.read()
        if not ret: break
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE))
        buf.append(rgb)
        if len(buf) == N_FRAMES:
            feats = cnn_forward(np.array(buf)).numpy()
            p = model.predict(feats[None, ...])[0]
            label = CLASSES[int(np.argmax(p))]
            score = float(np.max(p))
            buf = []
            cv2.putText(frame, f"{label}: {score:.2f}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
        cv2.imshow("Live", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'): break
    cap.release(); cv2.destroyAllWindows()